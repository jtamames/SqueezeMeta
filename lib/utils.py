"""
Part of the SqueezeMeta distribution. 25/03/2018 Original version, (c) Fernando Puente-Sánchez, CNB-CSIC.
python utilities for working with SqueezeMeta results
"""

from collections import defaultdict
from numpy import array, isnan, seterr
from os import listdir
from os.path import isdir
from json import loads
seterr(divide='ignore', invalid='ignore')

TAXRANKS = ('superkingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species')
TAXRANKS_SHORT = ('k', 'p', 'c', 'o', 'f', 'g', 's')



def parse_conf_file(project_path, override = {}):
    """
    Parse the configuration file containing all the information relevant fot a given SqueezeMeta project.
    Return a dictionary in which the key is the corresponding SqueezeMeta_conf.pl variable (with the leading sigil, as in "$aafile") and the value is the value of the said variable.
    Variable values can be overriden by providing a dictionary of structure {var_to_override: new_value}.
    """
    perlVars = {}
    for line in open('{}/SqueezeMeta_conf.pl'.format(project_path)):
        line = line.rsplit('#',1)[0] # Remove comment strings.
        if line.startswith('$'): # Is this a var definition?
            var, value = [x.strip(' \'\"') for x in line.strip().strip(';').split('=',1)]
            value = value if var not in override else override[var]
            perlVars[var] = value

    ### Define this bc it's funny to parse perl code with python.
    def perl_string_interpolation(string):
        if '$' in string:
            for var in perlVars:
                if var in string and not '\\{}'.format(var) in string: # The var is in the string, and the $ is not escaped like "\$"
                    string = string.replace(var, perl_string_interpolation(perlVars[var])) # Recursive interpolation.
        return string


    ### Back to work. Interpolate all the strings.
    for var, value in perlVars.items():
        perlVars[var] = perl_string_interpolation(value)

    return perlVars



def parse_mappingstat(mappingstat):
    """
    Parse a mappingstat file generated by SqueezeMeta.
    Return a tuple of numpy arrays containing the total reads and total bases for each sample.
    """
    # Parse weird final comment if mapping statistics are bad!
    # Check that sample order is the same as in the merged table!!

    total_reads = []
    total_bases = []
    
    for line in open(mappingstat):
        if not line.strip() or line.startswith('#'):
            continue
        line = line.strip().split('\t')
        total_reads.append(int(line[1]))
        total_bases.append(int(line[4]))

    return array(total_reads), array(total_bases)


def parse_orf_table(orf_table, total_reads, total_bases, nokegg, nocog, nopfam, trusted_only, ignore_unclassified_fun, custom_methods=None, data_dir=None, orfSet=None):
    """
    Parse a orftable generated by SqueezeMeta.
    Return:
        samplenames: list of sample names in the project
        orfs: a dictionary with the following keys
            abundances: a dictionary with orfs as keys, and a numpy array of abundances per sample
                        (samples sorted as in samplenames)
            copies: a dictionary with orfs as keys, and a numpy array of copies per sample
                    (in this case either 0 or 1 depending on whether the ORF is present or not
                     in each sample)
            length: a dictionary with orfs as keys, and a numpy array of total length per sample
                    (in this case either 0 or the length of the ORF,  depending on whether the ORF
                     is present or not in each sample)
            tpm: a dictionary with orfs as keys and a numpy array of tpms per sample.
        kegg, cog, pfam: dictionaries with the same structure as the orfs dictionary.
            the copies and length subdictionaries will contain the number of orfs from each kegg|cog|pfam
            in the different samples, or the aggregated length of each kegg|cog|pfam in the different
            samples, respectively. In the case of kegg and cog, the "info" subdictionary will
            contain function names and hierarchies.
        custom: for each custom annotation method, a dictionary with the same structure as kegg/cog.
        noCDSorfs: set of non-CDS (i.e. RNAs) ORFs, to be ignored when aggregating taxonomy.
        noCDScontigs: set of contigs containing no CDS ORFs, to be ignored when aggregating taxonomy.
    """

    orfs = {res: {}               for res in ('abundances', 'bases', 'coverages', 'copies', 'lengths')} # I know I'm being inconsistent with camelcase and underscores... ¯\_(ツ)_/¯
    kegg = {res: defaultdict(int) for res in ('abundances', 'bases', 'coverages', 'copies', 'lengths')}
    kegg['info'] = {}
    cog  = {res: defaultdict(int) for res in ('abundances', 'bases', 'coverages', 'copies', 'lengths')}
    cog['info'] = {}
    pfam = {res: defaultdict(int) for res in ('abundances', 'bases', 'coverages', 'copies', 'lengths')}
    custom = {method: {res: defaultdict(int) for res in ('abundances', 'bases', 'coverages', 'copies', 'lengths')} for method in custom_methods}
    [custom[method].update({'info': {'Unclassified': ['Unclassified']}}) for method in custom]
    noCDSorfs = set()
    allContigs = set()
    CDScontigs = set()
    funinfo = {}
    if data_dir:
        funinfo['kegg'] = {'Unclassified': ['Unclassified', 'Unclassified']}
        with open('{}/keggfun2.txt'.format(data_dir)) as infile:
            infile.readline() # Burn headers.
            for line in infile:
                fun_id, gene_name, fun_name, path = line.strip().split('\t')
                funinfo['kegg'][fun_id] = [fun_name, path]
        funinfo['cog'] = {'Unclassified': ['Unclassified', 'Unclassified']}
        with open('{}/coglist.txt'.format(data_dir)) as infile:
            infile.readline()
            for line in infile:
                line = line.strip().split('\t')
                if len(line) == 3:
                    fun_id, fun_name, path = line
                else: # UGH!
                    fun_id, fun_name = line
                    path = '{} (path not available)',format(fun_id)
                funinfo['cog'][fun_id] = [fun_name, path]



    ### Define helper functions.
    def update_dicts(funDict, colName, trusted_only):
        funIdx = idx[colName]
        # idx, abundances, coverages, copies, lengths and ignore_unclassified_fun are taken from the outer scope.
        if trusted_only and line[funIdx] and line[funIdx][-1] != '*': # Functions confirmed with the bestaver algorithm have a trailing asterisk.
            pass
        else:
            funs = line[funIdx].replace('*','')
            if colName != 'PFAM':
                funs = ['Unclassified'] if not funs else funs.strip(';').split(';') # So much fun!
            else: # PFAM has ID and description in the same field, and some descriptions have the ";" character we use for separation.
                funs = ['Unclassified'] if not funs else funs.strip(';').split('];')
                funs = ['{}]'.format(fun) if (i+1)<len(funs) else fun for i,fun in enumerate(funs)] # Add back the trailing "]" in all but the last function.
            for fun in funs:
                # If we have a multi-KO annotation, split counts between all KOs.
                funDict['abundances'][fun] += abundances / len(funs)
                funDict['bases'][fun]      += bases / len(funs)
                funDict['coverages'][fun]  += coverages / len(funs)
                funDict['copies'][fun]     += copies # We treat every KO as an individual smaller gene: less size, less reads, one copy.
                funDict['lengths'][fun]    += lengths / len(funs)
                if colName == 'KEGG ID':
                    info = funinfo['kegg'][fun] if fun in funinfo['kegg'] else ['{} (name not available)'.format(fun), '{} (path not available)'.format(fun)] 
                    funDict['info'][fun]    = info
                elif colName == 'COG ID':
                    info = funinfo['cog'][fun]  if fun in funinfo['cog']  else ['{} (name not available)'.format(fun), '{} (path not available)'.format(fun)]
                    funDict['info'][fun]    = info 
                else:
                    pass # we get info for custom annotations from the orftable, outside this function, PFAMs do not have associated info.


    def tpm(funDict):
        # Calculate reads per kilobase.    
        fun_avgLengths = {fun: funDict['lengths'][fun] / funDict['copies'][fun] for fun in funDict['lengths']} # NaN appears if a fun has no copies in a sample.
        fun_rpk = {fun: funDict['abundances'][fun] / (fun_avgLengths[fun]/1000) for fun in funDict['abundances']}
        if ignore_unclassified_fun:
            fun_rpk = {fun: rpk for fun, rpk in fun_rpk.items() if fun != 'Unclassified'}

        # Remove NaNs.
        for fun, rpk in fun_rpk.items():
            rpk[isnan(rpk)] = 0

        # Get tpm.    
        fun_tpm = normalize_abunds(fun_rpk, 1000000)
        return fun_tpm


    ### Do stuff.
    with open(orf_table) as infile:

        infile.readline() # Burn comment.
        header = infile.readline().strip().split('\t')
        idx =  {h: i for i,h in enumerate(header)}
        samples = [h for h in header if 'Raw read count' in h]
        samplesBases = [h for h in header if 'Raw base count' in h]
        samplesCov = [h for h in header if 'Coverage' in h]
        sampleNames = [s.replace('Raw read count ', '') for s in samples]

        for line in infile:
            line = line.strip().split('\t')
            orf = line[idx['ORF ID']]
            if orfSet and orf not in orfSet:
                continue
            length = line[idx['Length NT']]
            length = int(length) if length else 0 # Fix for rRNAs being in the ORFtable but having no length.
            if not length:
                print(line)
                continue # print and continue for debug info.
            contig = line[idx['Contig ID']]
            molecule = line[idx['Molecule']]
            allContigs.add(contig)
            if molecule == 'CDS':
                CDScontigs.add(contig)
            else:
                noCDSorfs.add(orf)
            abundances = array([int(line[idx[sample]]) for sample in samples])
            bases = array([int(line[idx[sample]]) for sample in samplesBases])
            coverages = array([float(line[idx[sample]]) for sample in samplesCov])
            copies = (abundances>0).astype(int) # 1 copy if abund>0 in that sample, else 0.
            lengths = length * copies   # positive if abund>0 in that sample, else 0.
            orfs['abundances'][orf] = abundances
            orfs['bases'][orf] = bases
            orfs['coverages'][orf] = coverages
            orfs['copies'][orf] = copies
            orfs['lengths'][orf] = lengths

            if not nokegg:
                update_dicts(kegg, 'KEGG ID', trusted_only)
            if not nocog:
                update_dicts(cog, 'COG ID', trusted_only)
            if not nopfam:
                update_dicts(pfam, 'PFAM', False) # PFAM are not subjected to the bestaver algorithm.
            for method in custom_methods:
                ID = line[idx[method]].replace('*', '')
                if ID:
                    custom[method]['info'][ID] = [ line[idx[method + ' NAME']] ]
                update_dicts(custom[method], method, trusted_only)

    # Calculate tpm.
    orfs['tpm'] = tpm(orfs)
    if not nokegg:
        kegg['tpm'] = tpm(kegg)
    if not nocog:
        cog['tpm']  = tpm(cog)
    if not nopfam:
        pfam['tpm'] = tpm(pfam)
    for mdic in custom.values():
        mdic['tpm'] = tpm(mdic)

    # If RecA/RadA is present (it should!), calculate copy numbers.
    if not nocog and 'COG0468' in cog['coverages']:
        RecA = cog['coverages']['COG0468']
        kegg['copyNumber'] = {k: cov/RecA for k, cov in kegg['coverages'].items()}
        cog['copyNumber']  = {k: cov/RecA for k, cov in cog['coverages'].items() }
        pfam['copyNumber'] = {k: cov/RecA for k, cov in pfam['coverages'].items()}
        for mdic in custom.values():
            mdic['copyNumber'] = {k: cov/RecA for k, cov in mdic['coverages'].items()}
    else:
        print('COG0468 (RecA/RadA) was not present in your data. This is weird, as RecA should be universal, so you probably just skipped COG annotation. Skipping copy number calculation...')

    noCDScontigs = allContigs - CDScontigs

    return sampleNames, orfs, kegg, cog, pfam, custom, noCDSorfs, noCDScontigs


def read_orf_names(orf_table):
    """
    Parse a orftable generated by SqueezeMeta.
    Return a set with the names of all the orfs.
    """
    with open(orf_table) as infile:
        infile.readline() # Burn comment.
        infile.readline() # Burn headers.
        return {line.split('\t')[0] for line in infile}


def parse_tax_table(tax_table, noCDS = set()):
    """
    Parse a fun3 taxonomy file from SqueezeMeta.
    Return:
        orf_tax: dictionary with orfs as keys and taxonomy list as values
        orf_tax_wranks: same, but with prefixes indicating each taxonomic rank (eg "p_<phylum>")
    """
    orf_tax = {}
    orf_tax_wranks = {}
    with open(tax_table) as infile:
        infile.readline() # Burn comment.
        for line in infile:
            line = line.strip().split('\t')
            if line[0] in noCDS: # This will actually not happen, since noCDS contigs are completely absent from the 06 results
                orf, tax, noClassString = line[0], 'n_noCDS', 'No CDS' # Add mock empty taxonomy, since there is not a CDS we can't classify the feature.
            elif len(line) == 1:
                orf, tax, noClassString = line[0], 'n_Unclassified', 'Unclassified' # Add mock empty taxonomy, as the feature is fully unclassified. 
            else:
                orf, tax = line
                noClassString = 'Unclassified' # I don't think it matters though
            orf_tax[orf], orf_tax_wranks[orf] = parse_tax_string(tax, noClassString)
    return orf_tax, orf_tax_wranks


def parse_contig_table(contig_table):
    """
    Parse a contig table generated by SqueezeMeta
    Return
        contig_abunds: dictionary with contigs as keys and a numpy array of contig abundances across 
                       samples as values.
        contig_tax: dictionary with contigs as keys and taxonomy lists as values
        contig_tax_wranks: same, but with prefixes indicating each taxonomic rank (eg "p_<phylum>")
    """
    contig_tax = {}
    contig_tax_wranks = {}
    contig_abunds = {}
    with open(contig_table) as infile:
        infile.readline() # Burn comment.
        header = infile.readline().strip().split('\t')
        idx =  {h: i for i,h in enumerate(header)}
        samples = [h for h in header if 'Raw read count' in h]
        sampleNames = [s.replace('Raw read count ', '') for s in samples]
        for line in infile:
            line = line.strip().split('\t')
            contig, tax = line[idx['Contig ID']], line[idx['Tax']]
            if not tax:
                tax = 'n_Unclassified' # Add mock empty taxonomy, as the read is fully unclassified.
            contig_tax[contig], contig_tax_wranks[contig] = parse_tax_string(tax)
            contig_abunds[contig] = array([int(line[idx[sample]]) for sample in samples])

    return contig_abunds, contig_tax, contig_tax_wranks


def parse_contig_tax(contiglog, noCDS = set()):
    """
    Parse a 09.*.contiglog file generated by SqueezeMeta
    Return
        contig_tax: dictionary with contigs as keys and taxonomy lists as values
        contig_tax_wranks: same, but with prefixes indicating each taxonomic rank (eg "p_<phylum>")
    """
    contig_tax = {}
    contig_tax_wranks = {}
    with open(contiglog) as infile:
        infile.readline() # Burn comment.
        for line in infile:
            # The allranks file contains one line per contig and rank.
            # We just overwrite the dictionary since the last one will be the most detailed.
            line = line.strip().split('\t')
            contig, tax = line[0], line[1]
            emptyClassString = 'Unclassified'
            if contig in noCDS:
                tax = 'n_noCDS' # Add mock empty taxonomy, since there is not a CDS we can't classify the feature.
                emptyClassString = 'No CDS'
            elif not tax:
                tax = 'n_Unclassified' # Add mock empty taxonomy, as the read is fully unclassified.
            try:
                contig_tax[contig], contig_tax_wranks[contig] = parse_tax_string(tax, emptyClassString)
            except:
                print(tax)
                raise
            
    return contig_tax, contig_tax_wranks


def parse_bin_table(bin_table):
    """
    Parse a contig table generated by SqueezeMeta
    Return
        bin_tpms: dictionary with contigs as keys and a numpy array of bin tpms across 
                       samples as values.
        bin_tax: dictionary with contigs as keys and taxonomy lists as values
        bin_tax_wranks: same, but with prefixes indicating each taxonomic rank (eg "p_<phylum>")
    """
    bin_tpm = {}
    bin_tax = {}
    bin_tax_wranks = {}
    with open(bin_table) as infile:
        infile.readline() # Brun comment.
        header =  infile.readline().strip().split('\t')
        idx =  {h: i for i,h in enumerate(header)}
        samples = [h for h in header if 'TPM' in h]
        sampleNames = [s.replace('TPM ', '') for s in samples]
        for line in infile:
            line = line.strip().split('\t')
            bin_, tax = line[idx['Bin ID']], line[idx['Tax']]
            if not tax or tax=='No consensus':
                tax = 'n_Unclassified' # Add mock empty taxonomy, as the read is fully unclassified.
            bin_tax[bin_], bin_tax_wranks[bin_] = parse_tax_string(tax)
            bin_tpm[bin_] = array([float(line[idx[sample]]) for sample in samples])
    return bin_tpm, bin_tax, bin_tax_wranks


def parse_tax_string(taxString, emptyClassString = 'Unclassified'):
    """
    Parse a taxonomy string as reported by the fun3 algorithm in SqueezeMeta
    Return:
        taxList: list of taxa sorted by rank
            [<superkingdom>, <phylum>, <class>, <order>, <family>, <genus>, <species>]
        taxList_wranks: same as taxlist, but with a prefix indicating each rank (eg "p_<phylum>")
    """
    taxDict = dict([r.split('_', 1) for r in taxString.strip(';').split(';')]) # We only preserve the last "no_rank" taxonomy, but we don't care.
    taxList = []
    lastRankFound = ''
    for rank in reversed(TAXRANKS_SHORT): # From species to superkingdom.
        if rank in taxDict:
            lastRankFound = rank
            taxList.append(taxDict[rank])
        elif lastRankFound:
            # This rank is not present,  but we have a classification at a lower rank.
            # This happens in the NCBI tax e.g. for some eukaryotes, they are classified at the class but not at the phylum level.
            # We inherit lower rank taxonomies, as we actually have classified that ORF.
            taxList.append('{} (no {} in NCBI)'.format(taxDict[lastRankFound], TAXRANKS[TAXRANKS_SHORT.index(rank)]))
        else:
            # Neither this or lower ranks were present. The ORF is not classified at this level.
            pass
    # Now add strings for the unclassified ranks.
    unclassString = 'Unclassified {}'.format(taxList[0]) if taxList else emptyClassString
    while len(taxList) < 7: # emptyClassString allows strings other than "Unclassified" to be used when we have no data
        taxList.insert(0, unclassString)

    # Reverse to retrieve the original order.
    taxList.reverse()

    # Generate comprehensive taxonomy strings.
    taxList_wranks = []
    for i, rank in enumerate(TAXRANKS_SHORT):
        newStr = '{}_{}'.format(rank, taxList[i])
        if i>0:
            newStr = '{};{}'.format(taxList_wranks[-1], newStr)
        taxList_wranks.append(newStr)

    return taxList, taxList_wranks


def aggregate_tax_abunds(orf_abunds, orf_tax, rankIdx, ignore = set()):
    """
    Aggregate the abundances of all orfs belonging to the same taxon at a given taxonomic rank.
    Return:
        tax_abunds: dictionary with taxa as keys, numpy array aggregated abundances as values
    """
    tax_abunds = defaultdict(int)
    for orf, abunds in orf_abunds.items():
        assert orf in orf_tax
        if orf in ignore:
            continue
        tax = orf_tax[orf][rankIdx]
        tax_abunds[tax] += abunds
    return tax_abunds


def normalize_abunds(abundDict, scale=1):
    """
    Normalize a dictionary of item abundances to a common scale.
    """
    abundPerSample = 0
    for row, abund in abundDict.items():
        abundPerSample += abund
    return {row: (scale * abund / abundPerSample) for row, abund in abundDict.items()}


def parse_fasta(fasta):
    """
    Parse a fasta file into a dictionary {header: sequence}
    """
    res = {}
    with open(fasta) as infile:
        header = ''
        seq = ''
        for line in infile:
            if line.startswith('>'):
                if header:
                    res[header] = seq
                    seq = ''
                header = line.strip().lstrip('>').split(' ')[0].split('\t')[0]
            else:
                seq += line.strip()
        res[header] = seq
    return res



def map_checkm_marker_genes(orf_table, checkm_dir):
    """
    Parse the results from checkm in order to identify which of our ORFs
     contained marker genes detected by CheckM
    Return:
        orf_markers: dictionary with ORFs as keys and sets of marker PFAMs as values
    """
    EVAL_THRESHOLD   = 1e-10 # Thresholds set as in https://github.com/Ecogenomics/CheckM/blob/master/checkm/defaultValues.py
    LENGTH_THRESHOLD = 0.7
    checkm_contigs = defaultdict(dict)
    for d in listdir(f'{checkm_dir}/bins'):
        if not isdir(f'{checkm_dir}/bins/{d}'):
            continue
        orf_se = {}
        with open(f'{checkm_dir}/bins/{d}/genes.gff') as infile:
            for line in infile:
                if line.startswith('#'):
                    continue
                line = line.strip().split('\t')
                contig, _, _, start, end, _, _, _, info = line
                ID = info.split('ID=')[1].split(';')[0]
                orf = contig + '_' + ID.split('_')[1]
                start, end = int(start), int(end)
                checkm_contigs[contig][orf] = {'start': start, 'end': end, 'PFAMs': set()}
    with open(f'{checkm_dir}/storage/marker_gene_stats.tsv') as infile:
        for line in infile:
            bin_, orfs = line.strip().split('\t')
            orfs = loads(orfs.replace('\'','"'))
            for orf, annots in orfs.items():
                # CheckM sometimes will treat two adjacent ORFs as a combined ORF (merging both names with "&&")
                #  This happens when the same marker gene is found in both
                #  In that case this is not treated as a duplication for the purpose of calculating contamination
                #   so it is counted only as one ORF
                #  We will annotate only the first ORF (to also count only one ORF)
                #  This will lead to wrong completeness estimations if only the second ORF is present in a subset
                #   but this is such a weird corner case (if we care about this we are most likely subsetting full contigs)
                #   that I deem it acceptable
                orf = orf.split('&&')[0]
                contig = orf.rsplit('_', 1)[0]
                for PFAM in annots:
                    checkm_contigs[contig][orf]['PFAMs'].add(PFAM)

    sqm_contigs = defaultdict(dict)
    with open(orf_table) as infile:
        for line in infile:
            if line.startswith('#') or line.startswith('ORF ID'):
                continue
            line = line.strip().split('\t')
            orf = line[0]
            contig, se = orf.rsplit('_', 1)
            start, end = [int(p) for p in se.split('-')]
            sqm_contigs[contig][orf] = {'start': start, 'end': end}


    # Map annotations from checkm ORFs (made by CheckM running prodigal on its own) to SQM ORFs (made by SQM running prodigal on its own)
    orf_markers = {orf: set() for contig, sqm_orfs in sqm_contigs.items() for orf in sqm_orfs}
    for contig, checkm_orfs in checkm_contigs.items():
        if contig not in sqm_contigs:
            continue
        sqm_orfs = sqm_contigs[contig]
        # Get the biggest position in the contig that is covered by a SQM orf
        maxs = max([data['start'] for data in sqm_orfs.values()])
        maxe = max([data['end'] for data in sqm_orfs.values()])
        maxp = max([maxs, maxe])
        # Create an array of positions in the contigs and the CheckM ORF to which they map to
        positions = [set() for i in range(maxp)]
        for orf, data in checkm_orfs.items():
            start, end = data['start'], data['end']
            if end < start:
                start, end = end, start
            if end > maxp:
                end = maxp
            for i in range(start-1, end): # switch from one to zero-indexing
                positions[i].add(orf)  # note that several orfs can overlap in the same position
        # Assign each SQM ORF to a CheckM ORF and inherit its PFAM annotation
        for orf, data in sqm_orfs.items():
            start, end = data['start'], data['end']
            if end < start:
                start, end = end, start
            checkmORFs = defaultdict(int)
            PFAMs = set()
            for i in range(start-1, end): # switch from one to zero-indexing
                for cmo in positions[i]:
                    checkmORFs[cmo] += 1
            if checkmORFs: # we found annotated equivalents in CheckM
                bestCheckmORF = max(checkmORFs, key=checkmORFs.get)
                overlap = checkmORFs[bestCheckmORF] / (end-start+1)
                checkm_orf_start, checkm_orf_end = checkm_orfs[bestCheckmORF]['start'], checkm_orfs[bestCheckmORF]['end']
                # Assimilate CheckM to SQM orf if overlap is more than 75%
                #  or the CheckM orf is contained in the SQM orf
                if overlap > 0.75 or (start <= checkm_orf_start and end >= checkm_orf_end):
                    PFAMs = checkm_orfs[bestCheckmORF]['PFAMs']
            orf_markers[orf] = PFAMs
    return orf_markers


def write_orf_seqs(orfs, aafile, fna_blastx, rrnafile, trnafile, outname):
    ### Create sequences file.
    # Load prodigal results.
    ORFseq = parse_fasta(aafile)
    # Load blastx results if required.
    if fna_blastx:
        ORFseq.update(parse_fasta(fna_blastx))
    if rrnafile:
        ORFseq.update(parse_fasta(rrnafile))
    if trnafile:
        ORFseq.update(parse_fasta(trnafile))
    # Write results.
    with open(outname, 'w') as outfile:
        outfile.write('ORF\tAASEQ\n')
        for ORF in orfs:
            outfile.write('{}\t{}\n'.format(ORF, ORFseq[ORF]))


def write_contig_seqs(contigfile, outname):
    contigseq = parse_fasta(contigfile)
    with open(outname, 'w') as outfile:
        outfile.write('CONTIG\tNTSEQ\n')
        for contig, seq in contigseq.items():
            outfile.write('{}\t{}\n'.format(contig, seq))


def write_row_dict(sampleNames, rowDict, outname):
    """
    rowDict is a dict where keys are features (rows) and values are arrays with the value for each column.
    """
    with open(outname, 'w') as outfile:
        outfile.write('\t{}\n'.format('\t'.join(sampleNames)))
        for row in sorted(rowDict):
            outfile.write('{}\t{}\n'.format(row, '\t'.join(map(str, rowDict[row]))))


def write_RDP_16S(rdp_table, outname):
    """
    Parse the results from step 2 and write them in two columns
    """
    with open(rdp_table) as infile, open(outname, 'w') as outfile:
        outfile.write(f'ORF\tTAX16S\n')
        for line in infile:
            if line.startswith('#'):
                continue
            line = line.strip().split('\t')
            if len(line) != 5: # some lines lack the final taxonomy string
                continue
            contig, *_, tax = line
            outfile.write(f'{contig}\t{tax}\n')

