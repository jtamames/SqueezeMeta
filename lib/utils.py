"""
Part of the SqueezeMeta distribution. 25/03/2018 Original version, (c) Fernando Puente-Sánchez, CNB-CSIC.
python utilities for working with SqueezeMeta results
"""

from collections import defaultdict
from numpy import array, isnan, seterr
from os import listdir
from os.path import isdir
seterr(divide='ignore', invalid='ignore')

TAXRANKS = ('superkingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species')
TAXRANKS_SHORT = ('k', 'p', 'c', 'o', 'f', 'g', 's')



def parse_conf_file(project_path, override = {}):
    """
    Parse the configuration file containing all the information relevant fot a given SqueezeMeta project.
    Return a dictionary in which the key is the corresponding SqueezeMeta_conf.pl variable (with the leading sigil, as in "$aafile") and the value is the value of the said variable.
    Variable values can be overriden by providing a dictionary of structure {var_to_override: new_value}.
    """
    perlVars = {}
    for line in open('{}/SqueezeMeta_conf.pl'.format(project_path)):
        line = line.rsplit('#',1)[0] # Remove comment strings.
        if line.startswith('$'): # Is this a var definition?
            var, value = [x.strip(' \'\"') for x in line.strip().strip(';').split('=',1)]
            value = value if var not in override else override[var]
            perlVars[var] = value

    ### Define this bc it's funny to parse perl code with python.
    def perl_string_interpolation(string):
        if '$' in string:
            for var in perlVars:
                if var in string and not '\\{}'.format(var) in string: # The var is in the string, and the $ is not escaped like "\$"
                    string = string.replace(var, perl_string_interpolation(perlVars[var])) # Recursive interpolation.
        return string


    ### Back to work. Interpolate all the strings.
    for var, value in perlVars.items():
        perlVars[var] = perl_string_interpolation(value)

    return perlVars



def parse_mappingstat(mappingstat):
    """
    Parse a mappingstat file generated by SqueezeMeta.
    Return a tuple of numpy arrays containing the total reads and total bases for each sample.
    """
    # Parse weird final comment if mapping statistics are bad!
    # Check that sample order is the same as in the merged table!!

    total_reads = []
    total_bases = []
    
    for line in open(mappingstat):
        if not line.strip() or line.startswith('#'):
            continue
        line = line.strip().split('\t')
        total_reads.append(int(line[1]))
        total_bases.append(int(line[4]))

    return array(total_reads), array(total_bases)


def parse_orf_table(orf_table, total_reads, total_bases, nokegg, nocog, nopfam, trusted_only, ignore_unclassified_fun, custom_methods=None, data_dir=None, orfSet=None):
    """
    Parse a orftable generated by SqueezeMeta.
    Return:
        samplenames: list of sample names in the project
        orfs: a dictionary with the following keys
            abundances: a dictionary with orfs as keys, and a numpy array of abundances per sample
                        (samples sorted as in samplenames)
            copies: a dictionary with orfs as keys, and a numpy array of copies per sample
                    (in this case either 0 or 1 depending on whether the ORF is present or not
                     in each sample)
            length: a dictionary with orfs as keys, and a numpy array of total length per sample
                    (in this case either 0 or the length of the ORF,  depending on whether the ORF
                     is present or not in each sample)
            tpm: a dictionary with orfs as keys and a numpy array of tpms per sample.
        kegg, cog, pfam: dictionaries with the same structure as the orfs dictionary.
            the copies and length subdictionaries will contain the number of orfs from each kegg|cog|pfam
            in the different samples, or the aggregated length of each kegg|cog|pfam in the different
            samples, respectively. In the case of kegg and cog, the "info" subdictionary will
            contain function names and hierarchies.
        custom: for each custom annotation method, a dictionary with the same structure as kegg/cog.
        noCDSorfs: set of non-CDS (i.e. RNAs) ORFs, to be ignored when aggregating taxonomy.
        noCDScontigs: set of contigs containing no CDS ORFs, to be ignored when aggregating taxonomy.
    """

    orfs = {res: {}               for res in ('abundances', 'bases', 'coverages', 'copies', 'lengths')} # I know I'm being inconsistent with camelcase and underscores... ¯\_(ツ)_/¯
    kegg = {res: defaultdict(int) for res in ('abundances', 'bases', 'coverages', 'copies', 'lengths')}
    kegg['info'] = {}
    cog  = {res: defaultdict(int) for res in ('abundances', 'bases', 'coverages', 'copies', 'lengths')}
    cog['info'] = {}
    pfam = {res: defaultdict(int) for res in ('abundances', 'bases', 'coverages', 'copies', 'lengths')}
    custom = {method: {res: defaultdict(int) for res in ('abundances', 'bases', 'coverages', 'copies', 'lengths')} for method in custom_methods}
    [custom[method].update({'info': {'Unclassified': ['Unclassified']}}) for method in custom]
    noCDSorfs = set()
    allContigs = set()
    CDScontigs = set()
    funinfo = {}
    if data_dir:
        funinfo['kegg'] = {'Unclassified': ['Unclassified', 'Unclassified']}
        with open('{}/keggfun2.txt'.format(data_dir)) as infile:
            infile.readline() # Burn headers.
            for line in infile:
                fun_id, gene_name, fun_name, path = line.strip().split('\t')
                funinfo['kegg'][fun_id] = [fun_name, path]
        funinfo['cog'] = {'Unclassified': ['Unclassified', 'Unclassified']}
        with open('{}/coglist.txt'.format(data_dir)) as infile:
            infile.readline()
            for line in infile:
                line = line.strip().split('\t')
                if len(line) == 3:
                    fun_id, fun_name, path = line
                else: # UGH!
                    fun_id, fun_name = line
                    path = '{} (path not available)',format(fun_id)
                funinfo['cog'][fun_id] = [fun_name, path]



    ### Define helper functions.
    def update_dicts(funDict, colName, trusted_only):
        funIdx = idx[colName]
        # idx, abundances, coverages, copies, lengths and ignore_unclassified_fun are taken from the outer scope.
        if trusted_only and line[funIdx] and line[funIdx][-1] != '*': # Functions confirmed with the bestaver algorithm have a trailing asterisk.
            pass
        else:
            funs = line[funIdx].replace('*','')
            if colName != 'PFAM':
                funs = ['Unclassified'] if not funs else funs.strip(';').split(';') # So much fun!
            else: # PFAM has ID and description in the same field, and some descriptions have the ";" character we use for separation.
                funs = ['Unclassified'] if not funs else funs.strip(';').split('];')
                funs = ['{}]'.format(fun) if (i+1)<len(funs) else fun for i,fun in enumerate(funs)] # Add back the trailing "]" in all but the last function.
            for fun in funs:
                # If we have a multi-KO annotation, split counts between all KOs.
                funDict['abundances'][fun] += abundances / len(funs)
                funDict['bases'][fun]      += bases / len(funs)
                funDict['coverages'][fun]  += coverages / len(funs)
                funDict['copies'][fun]     += copies # We treat every KO as an individual smaller gene: less size, less reads, one copy.
                funDict['lengths'][fun]    += lengths / len(funs)
                if colName == 'KEGG ID':
                    info = funinfo['kegg'][fun] if fun in funinfo['kegg'] else ['{} (name not available)'.format(fun), '{} (path not available)'.format(fun)] 
                    funDict['info'][fun]    = info
                elif colName == 'COG ID':
                    info = funinfo['cog'][fun]  if fun in funinfo['cog']  else ['{} (name not available)'.format(fun), '{} (path not available)'.format(fun)]
                    funDict['info'][fun]    = info 
                else:
                    pass # we get info for custom annotations from the orftable, outside this function, PFAMs do not have associated info.


    def tpm(funDict):
        # Calculate reads per kilobase.    
        fun_avgLengths = {fun: funDict['lengths'][fun] / funDict['copies'][fun] for fun in funDict['lengths']} # NaN appears if a fun has no copies in a sample.
        fun_rpk = {fun: funDict['abundances'][fun] / (fun_avgLengths[fun]/1000) for fun in funDict['abundances']}
        if ignore_unclassified_fun:
            fun_rpk = {fun: rpk for fun, rpk in fun_rpk.items() if fun != 'Unclassified'}

        # Remove NaNs.
        for fun, rpk in fun_rpk.items():
            rpk[isnan(rpk)] = 0

        # Get tpm.    
        fun_tpm = normalize_abunds(fun_rpk, 1000000)
        return fun_tpm


    ### Do stuff.
    with open(orf_table) as infile:

        infile.readline() # Burn comment.
        header = infile.readline().strip().split('\t')
        idx =  {h: i for i,h in enumerate(header)}
        samples = [h for h in header if 'Raw read count' in h]
        samplesBases = [h for h in header if 'Raw base count' in h]
        samplesCov = [h for h in header if 'Coverage' in h]
        sampleNames = [s.replace('Raw read count ', '') for s in samples]

        for line in infile:
            line = line.strip().split('\t')
            orf = line[idx['ORF ID']]
            if orfSet and orf not in orfSet:
                continue
            length = line[idx['Length NT']]
            length = int(length) if length else 0 # Fix for rRNAs being in the ORFtable but having no length.
            if not length:
                print(line)
                continue # print and continue for debug info.
            contig = line[idx['Contig ID']]
            molecule = line[idx['Molecule']]
            allContigs.add(contig)
            if molecule == 'CDS':
                CDScontigs.add(contig)
            else:
                noCDSorfs.add(orf)
            abundances = array([int(line[idx[sample]]) for sample in samples])
            bases = array([int(line[idx[sample]]) for sample in samplesBases])
            coverages = array([float(line[idx[sample]]) for sample in samplesCov])
            copies = (abundances>0).astype(int) # 1 copy if abund>0 in that sample, else 0.
            lengths = length * copies   # positive if abund>0 in that sample, else 0.
            orfs['abundances'][orf] = abundances
            orfs['bases'][orf] = bases
            orfs['coverages'][orf] = coverages
            orfs['copies'][orf] = copies
            orfs['lengths'][orf] = lengths

            if not nokegg:
                update_dicts(kegg, 'KEGG ID', trusted_only)
            if not nocog:
                update_dicts(cog, 'COG ID', trusted_only)
            if not nopfam:
                update_dicts(pfam, 'PFAM', False) # PFAM are not subjected to the bestaver algorithm.
            for method in custom_methods:
                ID = line[idx[method]].replace('*', '')
                if ID:
                    custom[method]['info'][ID] = [ line[idx[method + ' NAME']] ]
                update_dicts(custom[method], method, trusted_only)

    # Calculate tpm.
    orfs['tpm'] = tpm(orfs)
    if not nokegg:
        kegg['tpm'] = tpm(kegg)
    if not nocog:
        cog['tpm']  = tpm(cog)
    if not nopfam:
        pfam['tpm'] = tpm(pfam)
    for mdic in custom.values():
        mdic['tpm'] = tpm(mdic)

    # If RecA/RadA is present (it should!), calculate copy numbers.
    if not nocog and 'COG0468' in cog['coverages']:
        RecA = cog['coverages']['COG0468']
        kegg['copyNumber'] = {k: cov/RecA for k, cov in kegg['coverages'].items()}
        cog['copyNumber']  = {k: cov/RecA for k, cov in cog['coverages'].items() }
        pfam['copyNumber'] = {k: cov/RecA for k, cov in pfam['coverages'].items()}
        for mdic in custom.values():
            mdic['copyNumber'] = {k: cov/RecA for k, cov in mdic['coverages'].items()}
    else:
        print('COG0468 (RecA/RadA) was not present in your data. This is weird, as RecA should be universal, so you probably just skipped COG annotation. Skipping copy number calculation...')

    noCDScontigs = allContigs - CDScontigs

    return sampleNames, orfs, kegg, cog, pfam, custom, noCDSorfs, noCDScontigs


def read_orf_names(orf_table):
    """
    Parse a orftable generated by SqueezeMeta.
    Return a set with the names of all the orfs.
    """
    with open(orf_table) as infile:
        infile.readline() # Burn comment.
        infile.readline() # Burn headers.
        return {line.split('\t')[0] for line in infile}


def parse_tax_table(tax_table, noCDS = set()):
    """
    Parse a fun3 taxonomy file from SqueezeMeta.
    Return:
        orf_tax: dictionary with orfs as keys and taxonomy list as values
        orf_tax_wranks: same, but with prefixes indicating each taxonomic rank (eg "p_<phylum>")
    """
    orf_tax = {}
    orf_tax_wranks = {}
    with open(tax_table) as infile:
        infile.readline() # Burn comment.
        for line in infile:
            line = line.strip().split('\t')
            if line[0] in noCDS: # This will actually not happen, since noCDS contigs are completely absent from the 06 results
                orf, tax, noClassString = line[0], 'n_noCDS', 'No CDS' # Add mock empty taxonomy, since there is not a CDS we can't classify the feature.
            elif len(line) == 1:
                orf, tax, noClassString = line[0], 'n_Unclassified', 'Unclassified' # Add mock empty taxonomy, as the feature is fully unclassified. 
            else:
                orf, tax = line
                noClassString = 'Unclassified' # I don't think it matters though
            orf_tax[orf], orf_tax_wranks[orf] = parse_tax_string(tax, noClassString)
    return orf_tax, orf_tax_wranks


def parse_contig_table(contig_table):
    """
    Parse a contig table generated by SqueezeMeta
    Return
        contig_abunds: dictionary with contigs as keys and a numpy array of contig abundances across 
                       samples as values.
        contig_tax: dictionary with contigs as keys and taxonomy lists as values
        contig_tax_wranks: same, but with prefixes indicating each taxonomic rank (eg "p_<phylum>")
    """
    contig_tax = {}
    contig_tax_wranks = {}
    contig_abunds = {}
    with open(contig_table) as infile:
        infile.readline() # Burn comment.
        header = infile.readline().strip().split('\t')
        idx =  {h: i for i,h in enumerate(header)}
        samples = [h for h in header if 'Raw read count' in h]
        sampleNames = [s.replace('Raw read count ', '') for s in samples]
        for line in infile:
            line = line.strip().split('\t')
            contig, tax = line[idx['Contig ID']], line[idx['Tax']]
            if not tax:
                tax = 'n_Unclassified' # Add mock empty taxonomy, as the read is fully unclassified.
            contig_tax[contig], contig_tax_wranks[contig] = parse_tax_string(tax)
            contig_abunds[contig] = array([int(line[idx[sample]]) for sample in samples])

    return contig_abunds, contig_tax, contig_tax_wranks


def parse_contig_tax(contiglog, noCDS = set()):
    """
    Parse a 09.*.contiglog file generated by SqueezeMeta
    Return
        contig_tax: dictionary with contigs as keys and taxonomy lists as values
        contig_tax_wranks: same, but with prefixes indicating each taxonomic rank (eg "p_<phylum>")
    """
    contig_tax = {}
    contig_tax_wranks = {}
    with open(contiglog) as infile:
        infile.readline() # Burn comment.
        for line in infile:
            # The allranks file contains one line per contig and rank.
            # We just overwrite the dictionary since the last one will be the most detailed.
            line = line.strip().split('\t')
            contig, tax = line[0], line[1]
            emptyClassString = 'Unclassified'
            if contig in noCDS:
                tax = 'n_noCDS' # Add mock empty taxonomy, since there is not a CDS we can't classify the feature.
                emptyClassString = 'No CDS'
            elif not tax:
                tax = 'n_Unclassified' # Add mock empty taxonomy, as the read is fully unclassified.
            try:
                contig_tax[contig], contig_tax_wranks[contig] = parse_tax_string(tax, emptyClassString)
            except:
                print(tax)
                raise
            
    return contig_tax, contig_tax_wranks


def parse_bin_table(bin_table):
    """
    Parse a contig table generated by SqueezeMeta
    Return
        bin_tpms: dictionary with contigs as keys and a numpy array of bin tpms across 
                       samples as values.
        bin_tax: dictionary with contigs as keys and taxonomy lists as values
        bin_tax_wranks: same, but with prefixes indicating each taxonomic rank (eg "p_<phylum>")
    """
    bin_tpm = {}
    bin_tax = {}
    bin_tax_wranks = {}
    with open(bin_table) as infile:
        infile.readline() # Brun comment.
        header =  infile.readline().strip().split('\t')
        idx =  {h: i for i,h in enumerate(header)}
        samples = [h for h in header if 'TPM' in h]
        sampleNames = [s.replace('TPM ', '') for s in samples]
        for line in infile:
            line = line.strip().split('\t')
            bin_, tax = line[idx['Bin ID']], line[idx['Tax']]
            if not tax or tax=='No consensus':
                tax = 'n_Unclassified' # Add mock empty taxonomy, as the read is fully unclassified.
            bin_tax[bin_], bin_tax_wranks[bin_] = parse_tax_string(tax)
            bin_tpm[bin_] = array([float(line[idx[sample]]) for sample in samples])
    return bin_tpm, bin_tax, bin_tax_wranks


def parse_tax_string(taxString, emptyClassString = 'Unclassified'):
    """
    Parse a taxonomy string as reported by the fun3 algorithm in SqueezeMeta
    Return:
        taxList: list of taxa sorted by rank
            [<superkingdom>, <phylum>, <class>, <order>, <family>, <genus>, <species>]
        taxList_wranks: same as taxlist, but with a prefix indicating each rank (eg "p_<phylum>")
    """
    taxDict = dict([r.split('_', 1) for r in taxString.strip(';').split(';')]) # We only preserve the last "no_rank" taxonomy, but we don't care.
    taxList = []
    lastRankFound = ''
    for rank in reversed(TAXRANKS_SHORT): # From species to superkingdom.
        if rank in taxDict:
            lastRankFound = rank
            taxList.append(taxDict[rank])
        elif lastRankFound:
            # This rank is not present,  but we have a classification at a lower rank.
            # This happens in the NCBI tax e.g. for some eukaryotes, they are classified at the class but not at the phylum level.
            # We inherit lower rank taxonomies, as we actually have classified that ORF.
            taxList.append('{} (no {} in NCBI)'.format(taxDict[lastRankFound], TAXRANKS[TAXRANKS_SHORT.index(rank)]))
        else:
            # Neither this or lower ranks were present. The ORF is not classified at this level.
            pass
    # Now add strings for the unclassified ranks.
    unclassString = 'Unclassified {}'.format(taxList[0]) if taxList else emptyClassString
    while len(taxList) < 7: # emptyClassString allows strings other than "Unclassified" to be used when we have no data
        taxList.insert(0, unclassString)

    # Reverse to retrieve the original order.
    taxList.reverse()

    # Generate comprehensive taxonomy strings.
    taxList_wranks = []
    for i, rank in enumerate(TAXRANKS_SHORT):
        newStr = '{}_{}'.format(rank, taxList[i])
        if i>0:
            newStr = '{};{}'.format(taxList_wranks[-1], newStr)
        taxList_wranks.append(newStr)

    return taxList, taxList_wranks


def aggregate_tax_abunds(orf_abunds, orf_tax, rankIdx, ignore = set()):
    """
    Aggregate the abundances of all orfs belonging to the same taxon at a given taxonomic rank.
    Return:
        tax_abunds: dictionary with taxa as keys, numpy array aggregated abundances as values
    """
    tax_abunds = defaultdict(int)
    for orf, abunds in orf_abunds.items():
        assert orf in orf_tax
        if orf in ignore:
            continue
        tax = orf_tax[orf][rankIdx]
        tax_abunds[tax] += abunds
    return tax_abunds


def normalize_abunds(abundDict, scale=1):
    """
    Normalize a dictionary of item abundances to a common scale.
    """
    abundPerSample = 0
    for row, abund in abundDict.items():
        abundPerSample += abund
    return {row: (scale * abund / abundPerSample) for row, abund in abundDict.items()}


def parse_fasta(fasta):
    """
    Parse a fasta file into a dictionary {header: sequence}
    """
    res = {}
    with open(fasta) as infile:
        header = ''
        seq = ''
        for line in infile:
            if line.startswith('>'):
                if header:
                    res[header] = seq
                    seq = ''
                header = line.strip().lstrip('>').split(' ')[0].split('\t')[0]
            else:
                seq += line.strip()
        res[header] = seq
    return res


def map_checkm_marker_genes(orf_table, checkm_dir):
    """
    Parse the results from checkm in order to identify which of our ORFs
     contained marker genes detected by CheckM
    Return:
        orf_markers: dictionary with ORFs as keys and sets of marker PFAMs as values
    """
    checkm_contigs = defaultdict(dict)
    for d in listdir(f'{checkm_dir}/bins'):
        if not isdir(f'{checkm_dir}/bins/{d}'):
            continue
        orf_se = {}
        with open(f'{checkm_dir}/bins/{d}/genes.gff') as infile:
            for line in infile:
                if line.startswith('#'):
                    continue
                line = line.strip().split('\t')
                contig, _, _, start, end, _, _, _, info = line
                ID = info.split('ID=')[1].split(';')[0]
                orf = contig + '_' + ID.split('_')[1]
                start, end = int(start), int(end)
                orf_se[orf] = start, end
        with open(f'{checkm_dir}/bins/{d}/hmmer.analyze.txt') as infile:
            for line in infile:
                if line.startswith('#'):
                    continue
                line = [field for field in line.strip().split(' ') if field]
                orf = line[0]                  # here we are using the start and end of the ORF
                contig = orf.rsplit('_', 1)[0] # while in fairness we should use the start of end of the PFAM hit
                start, end = orf_se[orf]       # but it is a good enough approximation
                PFAM = line[4]
                if orf not in checkm_contigs[contig]:
                    checkm_contigs[contig][orf] = {'start': start, 'end': end, 'PFAMs': set([PFAM])}
                else:
                    checkm_contigs[contig][orf]['PFAMs'].add(PFAM) # we can have multiple annotations for the same ORF

    sqm_contigs = defaultdict(dict)
    with open(orf_table) as infile:
        for line in infile:
            if line.startswith('#') or line.startswith('ORF ID'):
                continue
            line = line.strip().split('\t')
            orf = line[0]
            contig, se = orf.rsplit('_', 1)
            start, end = [int(p) for p in se.split('-')]
            sqm_contigs[contig][orf] = {'start': start, 'end': end}


    # Map annotations from checkm ORFs (made by CheckM running prodigal on its own) to SQM ORFs (made by SQM running prodigal on its own)
    orf_markers = {orf: set() for contig, sqm_orfs in sqm_contigs.items() for orf in sqm_orfs}
    for contig, checkm_orfs in checkm_contigs.items():
        if contig not in sqm_contigs:
            continue
        sqm_orfs = sqm_contigs[contig]
        # Get the biggest position in the contig that is covered by a SQM orf
        maxs = max([data['start'] for data in sqm_orfs.values()])
        maxe = max([data['end'] for data in sqm_orfs.values()])
        maxp = max([maxs, maxe])
        # Create an array of positions in the contigs and the CheckM ORF to which they map to
        positions = [set() for i in range(maxp)]
        for orf, data in checkm_orfs.items():
            start, end = data['start'], data['end']
            if end < start:
                start, end = end, start
            if end > maxp:
                end = maxp
            for i in range(start-1, end): # switch from one to zero-indexing
                positions[i].add(orf)  # note that several orfs can overlap in the same position
        # Assign each SQM ORF to a CheckM ORF and inherit its PFAM annotation
        for orf, data in sqm_orfs.items():
            start, end = data['start'], data['end']
            if end < start:
                start, end = end, start
            checkmORFs = defaultdict(int)
            PFAMs = set()
            for i in range(start-1, end): # switch from one to zero-indexing
                for cmo in positions[i]:
                    checkmORFs[cmo] += 1
            if checkmORFs: # we found annotated equivalents in CheckM
                bestCheckmORF = max(checkmORFs, key=checkmORFs.get)
                overlap = checkmORFs[bestCheckmORF] / (end-start+1)
                if overlap > 0.75: # 75% or more overlap between the CheckM and SQM ORFs
                    PFAMs = checkm_orfs[bestCheckmORF]['PFAMs']
            orf_markers[orf] = PFAMs
    return orf_markers


def write_orf_seqs(orfs, aafile, fna_blastx, rrnafile, trnafile, outname):
    ### Create sequences file.
    # Load prodigal results.
    ORFseq = parse_fasta(aafile)
    # Load blastx results if required.
    if fna_blastx:
        ORFseq.update(parse_fasta(fna_blastx))
    if rrnafile:
        ORFseq.update(parse_fasta(rrnafile))
    if trnafile:
        ORFseq.update(parse_fasta(trnafile))
    # Write results.
    with open(outname, 'w') as outfile:
        outfile.write('ORF\tAASEQ\n')
        for ORF in orfs:
            outfile.write('{}\t{}\n'.format(ORF, ORFseq[ORF]))


def write_contig_seqs(contigfile, outname):
    contigseq = parse_fasta(contigfile)
    with open(outname, 'w') as outfile:
        outfile.write('CONTIG\tNTSEQ\n')
        for contig, seq in contigseq.items():
            outfile.write('{}\t{}\n'.format(contig, seq))


def write_row_dict(sampleNames, rowDict, outname):
    """
    rowDict is a dict where keys are features (rows) and values are arrays with the value for each column.
    """
    with open(outname, 'w') as outfile:
        outfile.write('\t{}\n'.format('\t'.join(sampleNames)))
        for row in sorted(rowDict):
            outfile.write('{}\t{}\n'.format(row, '\t'.join(map(str, rowDict[row]))))

